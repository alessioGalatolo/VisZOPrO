from abc import abstractmethod, ABC
from dataclasses import fields
import os
from typing import Any
from copy import deepcopy
import argparse
from functools import partial

from safetensors import safe_open
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainerCallback,
    TrainingArguments,
    AutoModelForSequenceClassification,
)
from datasets import load_dataset, Dataset
from peft import (
    LoraConfig,
    get_peft_model,
    AutoPeftModelForCausalLM,
    AutoPeftModelForSequenceClassification,
)
from peft.utils.save_and_load import (
    get_peft_model_state_dict,
    set_peft_model_state_dict,
)
from trl import (
    PPOTrainer,
    PPOConfig,
    SFTTrainer,
    RewardConfig,
    OnlineDPOConfig,
    OnlineDPOTrainer,
    RLOOConfig,
    RLOOTrainer,
)
from trl.trainer import ConstantLengthDataset
from trl.trainer.utils import first_true_indices
from accelerate.utils import gather_object

from dataset_utils import generate_pairs, judge_samples, DatasetYielder, process4pl, generate_both_pairs
from extract_preference_pairs import extract_dialogue
from zopro import ZORLOOTrainer
from reward_trainer_general import RewardTrainerGeneral


def base2specific_config(base: TrainingArguments, specific: Any, name: str, **kwargs) -> TrainingArguments:
    # specific has more attributes than TrainingArguments
    # Copy all attributes from TrainingArguments to specific
    parent_fields = {f.name for f in fields(TrainingArguments)}
    parent_dict = {f: getattr(base, f) for f in parent_fields}
    specific_config = specific(os.path.join(parent_dict["output_dir"], name), **kwargs)
    for f in fields(specific):
        if f.name in parent_fields and f.name not in kwargs:
            setattr(specific_config, f.name, parent_dict[f.name])
    specific_config.run_name += '_' + name
    specific_config.output_dir = os.path.join(parent_dict["output_dir"], name)
    return specific_config


class SaveLoraWeightsCallback(TrainerCallback):
    # callback for trainer, saves the model weights every 5% of the training steps
    def __init__(self, model, log_dir: str, iteration: int = 0, total_training_steps: int = None, log_frequency: int = 5, adapter_name: str = "default"):
        self.model = model
        self.log_dir = log_dir
        self.iteration = iteration
        divider = 100 // log_frequency
        # Log every 5% of the training steps
        self.logging_steps = (total_training_steps // divider) + 1 if total_training_steps is not None else 5
        self.adapter_name = adapter_name

    def on_step_end(self, args, state, control, **kwargs):
        epoch = state.epoch
        step = state.global_step
        if (step+1) % self.logging_steps != 0:
            return
        log_file = os.path.join(self.log_dir, f'lora_weights_iteration_{self.iteration}_epoch_{int(epoch)}_step_{step}')
        self.model.save_pretrained(log_file, selected_adapters=[self.adapter_name])


# used to replace the forward method of the reward model
# to store the input_ids generated by the policy model.
# This way, they can be used to further fine-tune the reward model
# based on the distribution shift of the policy model. It also stores the
# scores of the reward model to focus on low confidence examples.
def store_input_ids_and_forward(
    self,
    input_ids=None,
    attention_mask=None,
    position_ids=None,
    past_key_values=None,
    inputs_embeds=None,
    labels=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
    generation_length=53,
):
    # Call the original forward method
    result = self.original_forward(
        input_ids=input_ids,
        attention_mask=attention_mask,
        position_ids=position_ids,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        labels=labels,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
    # this method is called both during policy training and reward training
    # if reward training, do not store input_ids
    if self.reward_training:
        return result
    if self.prompt_ids:
        # current input could have already been stored
        old_input = [x+y for x, y in zip(self.prompt_ids[-input_ids.size(0):], self.answer_ids[-input_ids.size(0):])]
        if old_input == input_ids.clone().detach().cpu().tolist():
            return result
    # get uncertainty of the reward model
    context_length = input_ids.size(1) - generation_length  # this assumes no early stopping when encountering eos_token
    reward_logits = self.score(result.hidden_states[-1])
    sequence_lengths = first_true_indices(input_ids[:, context_length:] == self.config.pad_token_id) - 1 + context_length

    self.prompt_ids.extend(input_ids[:, :context_length].clone().detach().cpu().tolist())
    self.answer_ids.extend(input_ids[:, context_length:].clone().detach().cpu().tolist())
    # will be used to filter out the generations (not the best way to get uncertainty)
    scores = reward_logits[
        torch.arange(reward_logits.size(0), device=reward_logits.device),
        sequence_lengths,
    ].squeeze(-1)
    self.stored_scores.extend(scores.clone().detach().cpu().tolist())
    return result


class PLLoRATrainer(ABC):
    """
    Trainer for Policy Learning using LoRA with weights logging.
    This trainer includes training of SFT, reward model and policy (iteratively).
    """
    def __init__(
        self,
        model_name: str = 'meta-llama/Llama-3.2-1B',
        dataset_name: str = 'Anthropic/hh-rlhf',
        task: str = 'chat',
        lora_r: int = 64,
        lora_alpha: int = None,  # if none, it is set to 2*lora_r
        max_length: int = 1024,
        log_dir: str = './logs',
        batch_size: int = 2,
        pl_epochs: int = 3,  # policy learning epochs
        intraepoch_refinement: bool = False,  # whether the refinement of the RM should be done within the epoch (or at the end)
        iteration_percentage: float = 0.05,  # if above is true, this is the percentage of the epoch after which to refine the RM
        keep_rm_close: bool = True,  # whether to merge the RM of current iteration with the previous one (helps with stability)
        keep_rm_alpha: float = 0.5,  # how much to weigh the previous RM
        lora_log_frequency: int = 5,  # save lora weights every 5% of the training steps FIXME: use float instead (similar to iteration percentage)
        learning_rate: float = 5e-5,
        generation_length: int = 53,
        **kwargs
    ):
        self.task = task
        self.generation_length = generation_length
        self.iteration_percentage = iteration_percentage
        self.lora_log_frequency = lora_log_frequency
        self.pl_epochs = pl_epochs
        self.intraepoch_refinement = intraepoch_refinement
        self.keep_rm_close = keep_rm_close
        self.keep_rm_alpha = keep_rm_alpha
        self.general_folder = log_dir  # this contains general models such as sft and reward_0
        self.log_dir = log_dir
        run_name = os.environ.get('RUN_NAME', "")
        if run_name:
            self.log_dir = os.path.join(self.log_dir, run_name)
        os.makedirs(self.log_dir, exist_ok=True)

        self.max_length = max_length
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha if lora_alpha is not None else 2*lora_r
        # Model and tokenizer setup
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")

        # FIXME: can perhaps have a better template
        self.tokenizer.chat_template = "{{- bos_token }}\n{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}"
        if task == 'chat':
            self.tokenizer.chat_template += "{{ '<|im_start|>assistant\n' }}{% endif %}"
        elif task == 'summarization':
            self.tokenizer.chat_template += "{{ '<|im_start|>Summary\n' }}{% endif %}"
        elif task == 'MT':
            self.tokenizer.chat_template += "{{ '<|im_start|>Translation\n' }}{% endif %}"
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id  # FIXME

        self.base_model = model_name

        self.sft_model = None
        self.policy_model = None
        self.reward_model = None

        # this is a common config for all training
        # specific values will be set in the specific methods
        self.base_training_config = TrainingArguments(
            output_dir=self.log_dir,
            eval_strategy="steps",
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=64 // batch_size,
            bf16=True,
            num_train_epochs=3,
            learning_rate=learning_rate,
            run_name=os.environ.get('RUN_NAME', ""),
            save_steps=50,
            logging_steps=50,
            save_total_limit=5,
            optim='adamw_torch',
            eval_steps=500,
            report_to="wandb",
        )
        self.dataset_name = dataset_name

        # FIXME: this is really ugly but need change of logic to fix it
        if dataset_name == "openai/summarize_from_feedback":
            self.dataset = load_dataset(dataset_name, "comparisons")
        elif dataset_name == "wmt/wmt20_mlqe_task1":
            self.dataset = load_dataset(dataset_name, "en-de")
        else:
            self.dataset = load_dataset(self.dataset_name)

    # initialize the SFT model if not already done and calls the training method
    def _sft_setup(self):
        if self.reward_model is not None:
            self.reward_model.to('cpu')

        sft_path = os.path.join(self.general_folder, 'final_sft_model')
        if os.path.exists(sft_path):
            print(f"Loading SFT model from {sft_path}")
            # This is necessary if the model is saved with save_pretrained(..., selected_adapters=...)
            sft_path = os.path.join(sft_path, "sft")
            self.sft_model = AutoPeftModelForCausalLM.from_pretrained(sft_path, torch_dtype="auto", adapter_name="sft")
            return
        self.sft_model = AutoModelForCausalLM.from_pretrained(self.base_model, torch_dtype="auto")
        self.sft_model = get_peft_model(self.sft_model, LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            target_modules="all-linear",
            task_type="CAUSAL_LM",
            lora_dropout=0.1,
            bias='none'
        ), adapter_name="sft")
        self.sft_model.config.pad_token_id = self.tokenizer.pad_token_id
        self._sft_train()
        self.sft_model.save_pretrained(sft_path, selected_adapters=["sft"])

    # train the SFT model
    def _sft_train(self):
        sft_config = deepcopy(self.base_training_config)
        sft_config.run_name += '_sft'
        sft_config.output_dir = os.path.join(sft_config.output_dir, 'sft')

        extract_fn = partial(
            extract_dialogue,
            example_name=self.dataset_name,
            task=self.task
        )

        sft_train_data = self.dataset['train'].map(extract_fn, num_proc=16)
        test_name = 'test' if 'test' in self.dataset else 'validation'
        sft_eval_data = self.dataset[test_name].map(extract_fn, num_proc=16)

        def formatting_func(x):
            if "chosen" in x:
                return self.tokenizer.apply_chat_template(x['prompt'] + x['chosen'], tokenize=False, add_generation_prompt=True)
            elif "completion" in x:
                return self.tokenizer.apply_chat_template(x['prompt'] + x['completion'], tokenize=False, add_generation_prompt=True)
            else:
                raise ValueError("No 'chosen' or 'completion' in the dataset")

        # FIXME: currently caps the input
        sft_train_data = ConstantLengthDataset(
            self.tokenizer,
            sft_train_data,
            seq_length=self.max_length,
            formatting_func=formatting_func
        )
        sft_eval_data = ConstantLengthDataset(
            self.tokenizer,
            sft_eval_data,
            seq_length=self.max_length,
            formatting_func=formatting_func
        )
        # FIXME: should use DataCollatorForCompletionOnlyLM ?
        sft_trainer = SFTTrainer(
            model=self.sft_model,
            args=sft_config,
            processing_class=self.tokenizer,
            train_dataset=sft_train_data,
            eval_dataset=sft_eval_data,
        )
        print("Training SFT model")
        sft_trainer.train()
        print("SFT model training complete")

    # initialize the reward model if not already done and calls the training method
    def _reward_setup(self):
        if self.sft_model is not None:
            self.sft_model.to('cpu')
        if self.policy_model is not None:
            self.policy_model.to('cpu')
        reward_path = os.path.join(self.general_folder, 'reward_model_iter_0')
        if self.reward_model is None:
            # load from checkpoint if available
            if os.path.exists(reward_path):
                print("Loading reward model from checkpoint")
                # this is necessary in new code (see sft comment)
                reward_path = os.path.join(reward_path, "reward")
                self.reward_model = AutoPeftModelForSequenceClassification.from_pretrained(reward_path, num_labels=1, torch_dtype="auto", adapter_name="reward")
                self.reward_model.config.pad_token_id = self.tokenizer.pad_token_id
                return
            self.reward_model = AutoModelForSequenceClassification.from_pretrained(self.base_model, num_labels=1, torch_dtype="auto")
            self.reward_model = get_peft_model(self.reward_model, LoraConfig(
                r=self.lora_r,
                lora_alpha=self.lora_alpha,
                target_modules="all-linear",
                task_type="SEQ_CLS",
                lora_dropout=0.1,
                bias='none'
            ), adapter_name="reward")
            self.reward_model.config.pad_token_id = self.tokenizer.pad_token_id
        self._reward_train(0)
        self.reward_model.save_pretrained(reward_path, selected_adapters=["reward"])

    # train the reward model
    # this is called once before policy training and then iteratively
    def _reward_train(
        self,
        iteration: int,
        train_data=None,  # None for iteration 0, then gets the data from the policy model
        resume=False
    ):
        reward_config = base2specific_config(
            base=self.base_training_config,
            specific=RewardConfig,
            name="reward",
            max_length=self.max_length,
            remove_unused_columns=True,
        )
        callbacks = None
        if train_data is None:
            # this probably means we are in iteration 0
            train_data = self.dataset['train'].map(extract_dialogue, fn_kwargs={'example_name': self.dataset_name, 'task': self.task}, load_from_cache_file=False)
            test_name = 'test' if 'test' in self.dataset else 'validation'
            eval_data = self.dataset[test_name].map(extract_dialogue, fn_kwargs={'example_name': self.dataset_name, 'task': self.task}, load_from_cache_file=False)
        else:
            # setup the callback to save the model weights
            world_size = torch.cuda.device_count()
            reduced_training_steps = len(train_data) // 64 // world_size
            if self.intraepoch_refinement:
                reduced_training_steps *= 20  # policy model is logged less times per iteration
            # save the model weights every 5% of the training steps
            callbacks = None
            if self.lora_log_frequency is not None:
                callbacks = [
                    SaveLoraWeightsCallback(
                        self.reward_model,
                        reward_config.output_dir,
                        iteration=iteration,
                        total_training_steps=reduced_training_steps,
                        log_frequency=self.lora_log_frequency,
                        adapter_name="reward"
                    )
                ]
            eval_data = None  # we don't have this # FIXME: could split dataset

        if resume and os.path.exists(os.path.join(self.log_dir, f'reward_model_iter_{iteration}')):
            print("Loading reward model from checkpoint for iteration ", iteration)
            self.reward_model = (
                AutoPeftModelForSequenceClassification.from_pretrained(
                    os.path.join(self.log_dir, f"reward_model_iter_{iteration}", "reward"),
                    num_labels=1,
                    torch_dtype="auto",
                    adapter_name="reward"
                )
            )
            self.reward_model.config.pad_token_id = self.tokenizer.pad_token_id
            self._prepare_reward4pl()
        else:
            print("Training reward model, iteration ", iteration)
            # this is used to set requires_grad to True for the adapter weights
            self.reward_model.set_adapter("reward")
            # set reward training to True to avoid storing input_ids
            getattr(self.reward_model, self.reward_model.base_model_prefix).reward_training = True
            reward_trainer = RewardTrainerGeneral(
                model=self.reward_model,
                args=reward_config,
                processing_class=self.tokenizer,
                train_dataset=train_data,
                eval_dataset=eval_data,
                callbacks=callbacks
            )
            reward_trainer.train()
            print("Reward model training complete")
            self.reward_model.save_pretrained(os.path.join(self.log_dir, f'reward_model_iter_{iteration}'), selected_adapters=["reward"])
        getattr(self.reward_model, self.reward_model.base_model_prefix).reward_training = False
        # FIXME: should keep rm close be before saving? # YES! But the code has been adapted for this mistake already
        # if rm model should not change much, get the previous iteration and merge the weights
        if self.keep_rm_close and iteration > 0:
            self.reward_model.to('cpu')
            # FIXME: this can be also achieved by training a new adapter and fusing the two at the end
            old_model = AutoPeftModelForSequenceClassification.from_pretrained(
                os.path.join(self.log_dir, f"reward_model_iter_{iteration - 1}", "reward"),
                num_labels=1,
                torch_dtype="auto",
                adapter_name="reward"
            )
            original_state_dict = get_peft_model_state_dict(old_model, adapter_name="reward")
            new_state_dict = get_peft_model_state_dict(self.reward_model, adapter_name="reward")
            merged_state_dict = {}
            alpha = self.keep_rm_alpha
            for key, value in original_state_dict.items():
                merged_state_dict[key] = alpha * value + (1 - alpha) * new_state_dict[key]
            set_peft_model_state_dict(self.reward_model, merged_state_dict, adapter_name="reward")

    def _prepare_reward4pl(self):
        # Replace forward of reward model with custom one that stores input ids
        # Get the backbone model
        lm_backbone = getattr(self.reward_model, self.reward_model.base_model_prefix)

        # Store the original forward method
        lm_backbone.original_forward = lm_backbone.forward

        # Replace the forward method with the custom one
        lm_backbone.forward = partial(store_input_ids_and_forward.__get__(lm_backbone), generation_length=self.generation_length)

        # Now add an attribute to the backbone to store the input ids
        lm_backbone.stored_scores = []
        lm_backbone.prompt_ids = []
        lm_backbone.answer_ids = []
        lm_backbone.reward_training = False

    # prepare policy for training. Does NOT start the training (FIXME: bad logic if compared to other methods)
    def _pl_setup(self):
        if self.policy_model is None:
            if self.sft_model is None:
                self._sft_setup()
            self.policy_model = self.sft_model
            self.policy_model.add_adapter("pl", self.policy_model.active_peft_config)
            self.policy_model.set_adapter("pl")
            lora_state_dict = get_peft_model_state_dict(self.policy_model, adapter_name="sft")
            set_peft_model_state_dict(self.policy_model, lora_state_dict, adapter_name="pl")
        if self.reward_model is None:
            self._reward_setup()
        self.policy_model.eval()
        self.policy_model.pad_token_id = self.tokenizer.pad_token_id
        self.reward_model.eval()

        self._prepare_reward4pl()

    # subclass and define this method for e.g. PPO, DPO, etc. returns a trainer that can be called with .train()
    @abstractmethod
    def _pl_trainer_setup(self, train_data, eval_data, iteration, total_training_steps):
        ...

    def train(self, resume=False):
        os.environ["WANDB_PROJECT"] = "pl-lora-training"

        self._sft_setup()
        self._reward_setup()
        self._pl_setup()

        # reduce max length so that input+generation fits into the reward model
        max_length = self.max_length - self.generation_length
        pl_train_data = self.dataset['train'].map(
            process4pl,
            num_proc=16,
            load_from_cache_file=False,  # this is necessary if process4pl changes (otherwise old data is used)
            fn_kwargs={'tokenizer': self.tokenizer, 'max_length': max_length+1, "example_name": self.dataset_name, "task": self.task})
        columns2remove = pl_train_data.column_names
        # remove everything, keep only input_ids and prompt
        columns2remove.remove('input_ids')
        columns2remove.remove('prompt')
        pl_train_data = pl_train_data.map(remove_columns=columns2remove)
        test_name = 'test' if 'test' in self.dataset else 'validation'
        pl_eval_data = self.dataset[test_name].map(
            process4pl,
            num_proc=16,
            load_from_cache_file=False,  # this is necessary if process4pl changes (otherwise old data is used)
            fn_kwargs={'tokenizer': self.tokenizer, 'max_length': max_length+1, "example_name": self.dataset_name, "task": self.task})
        pl_eval_data = pl_eval_data.map(remove_columns=columns2remove)
        # filter out examples that are too long
        pl_train_data = pl_train_data.filter(lambda x: len(x['input_ids']) <= max_length)
        pl_eval_data = pl_eval_data.filter(lambda x: len(x['input_ids']) <= max_length)

        # this takes the dataset and yields chunks accounting for intraepoch_refinement
        dataset_yielder = DatasetYielder(pl_train_data, pl_eval_data, self.pl_epochs, self.intraepoch_refinement, self.iteration_percentage)
        for iteration, (train_data, eval_data) in enumerate(dataset_yielder):
            if resume:
                # check if log_dir contains the last pl checkpoint and relative data
                if not os.path.exists(os.path.join(self.log_dir, f'policy_model_iteration_{iteration}')):
                    print("No policy checkpoint found for iteration ", iteration, ". Starting actual training.")
                    resume = False
                elif not os.path.exists(os.path.join(self.log_dir, f'reward_data_iteration_{iteration}.csv')):
                    print("No reward data found for iteration ", iteration, ". Starting actual training.")
                    resume = False
                else:
                    print("Loading policy model and data from checkpoint for iteration ", iteration)
                    self.policy_model.load_adapter(model_id=os.path.join(self.log_dir, f'policy_model_iteration_{iteration}', "pl"), adapter_name="pl")
                    self.policy_model.set_adapter("pl")
                    self.policy_model.pad_token_id = self.tokenizer.pad_token_id
                    reward_data = Dataset.from_csv(os.path.join(self.log_dir, f'reward_data_iteration_{iteration}.csv'), escapechar='\\', quoting=1, quotechar='"')
                    self._reward_train(iteration=iteration+1, train_data=reward_data, resume=True)
                    continue

            # run pl, refine reward model

            # this is used to set requires_grad to True for the adapter weights
            self.policy_model.set_adapter("pl")
            for param in self.reward_model.parameters():
                param.requires_grad = False
            pl_trainer = self._pl_trainer_setup(
                train_data=train_data,
                eval_data=eval_data,
                iteration=iteration,
                total_training_steps=len(train_data) // 64 // torch.cuda.device_count()
            )
            print("Training policy model")
            pl_trainer.train()
            print("Policy model training complete")
            self.reward_model.to('cpu')
            torch.cuda.empty_cache()

            # at this point we have stored inputs + one completion from the policy model
            # to refine the reward model, we need to generate the other completion
            # and then judge the pairs to see which one is better
            print("Generating completions for pairwise comparison")
            if not self.reward_model.prompt_ids:
                # prompt ids were lost, probably due to resumes
                # generate both pairs instead of just one
                data2be_annotated = generate_both_pairs(
                    self.policy_model,
                    self.tokenizer,
                    train_data['prompt'],
                    batch_size=self.base_training_config.per_device_eval_batch_size,
                    generation_length=self.generation_length
                )
            else:
                # NOTE: when running on multiple GPUs, the data will be unique for each GPU
                data2be_annotated = generate_pairs(
                    self.policy_model,
                    self.tokenizer,
                    self.reward_model.prompt_ids,
                    self.reward_model.answer_ids,
                    self.reward_model.stored_scores,
                    batch_size=self.base_training_config.per_device_eval_batch_size,
                    generation_length=self.generation_length
                )

            # if world_size == 4:
            #     model_name = "meta-llama/Llama-3.3-70B-Instruct"
            # else:

            # load small model as judge_samples assumes model fits on a single GPU (TODO)
            model_name = "meta-llama/Llama-3.1-8B-Instruct"
            # annotate the generated samples with an external judge
            print("Judging samples with external model")
            reward_data = judge_samples(model_name=model_name, samples=data2be_annotated, batch_size=self.base_training_config.per_device_eval_batch_size)

            # in distributed environments reward_data will be different for each GPU
            # so we need to gather them
            reward_data = gather_object(reward_data)
            reward_data = Dataset.from_list(reward_data)
            # save just in case
            reward_data.to_csv(os.path.join(self.log_dir, f'reward_data_iteration_{iteration}.csv'), escapechar='\\', quoting=1, quotechar='"')
            # also save policy (this is not done before as it is useless without the new reward data)
            self.policy_model.save_pretrained(os.path.join(self.log_dir, f'policy_model_iteration_{iteration}'), selected_adapters=["pl"])
            self._reward_train(iteration=iteration+1, train_data=reward_data)
            # clear the stored data
            lm_backbone = getattr(self.reward_model, self.reward_model.base_model_prefix)
            lm_backbone.prompt_ids = []
            lm_backbone.answer_ids = []
            lm_backbone.stored_scores = []


class PPO(PLLoRATrainer):
    def _pl_trainer_setup(self, train_data, eval_data, iteration, total_training_steps):
        # now the data contains the prompt used by other methods, drop it
        train_data = train_data.map(
            remove_columns=['prompt'],
        )
        eval_data = eval_data.map(
            remove_columns=['prompt'],
        )
        pl_config = base2specific_config(
            base=self.base_training_config,
            specific=PPOConfig,
            name="ppo",
            local_rollout_forward_batch_size=self.base_training_config.per_device_eval_batch_size,
            num_ppo_epochs=1,
            response_length=self.generation_length,
            model_adapter_name="pl",
            ref_adapter_name="sft"
        )
        pl_callbacks = None
        if self.lora_log_frequency is not None:
            pl_callbacks = [
                SaveLoraWeightsCallback(
                    self.policy_model,
                    pl_config.output_dir,
                    iteration=iteration,
                    total_training_steps=total_training_steps,  # use the whole dataset to avoid too frequent logging
                    log_frequency=self.lora_log_frequency,
                    adapter_name="pl"
                )
            ]
        ppo_trainer = PPOTrainer(
            model=self.policy_model,
            value_model=self.reward_model,
            reward_model=self.reward_model,
            processing_class=self.tokenizer,
            ref_model=None,
            args=pl_config,
            callbacks=pl_callbacks,
            train_dataset=train_data,
            eval_dataset=eval_data,
        )
        return ppo_trainer


class DPO(PLLoRATrainer):
    def _pl_trainer_setup(self, train_data, eval_data, iteration, total_training_steps):
        train_data = train_data.map(
            remove_columns=['input_ids'],
        )
        eval_data = eval_data.map(
            remove_columns=['input_ids'],
        )
        pl_config = base2specific_config(
            base=self.base_training_config,
            specific=OnlineDPOConfig,
            name="dpo",
            logging_steps=5,  # by default DPO logs less frequently than PPO, RLOO, etc
            max_new_tokens=self.generation_length,
            num_train_epochs=1,
            model_adapter_name="pl",
            ref_adapter_name="sft"
        )
        pl_callbacks = None
        if self.lora_log_frequency is not None:
            pl_callbacks = [
                SaveLoraWeightsCallback(
                    self.policy_model,
                    pl_config.output_dir,
                    iteration=iteration,
                    total_training_steps=total_training_steps,
                    log_frequency=self.lora_log_frequency,
                    adapter_name="pl"
                )
            ]
        dpo_trainer = OnlineDPOTrainer(
            model=self.policy_model,
            reward_model=self.reward_model,
            processing_class=self.tokenizer,
            reward_processing_class=self.tokenizer,
            args=pl_config,
            train_dataset=train_data,
            eval_dataset=eval_data,
            callbacks=pl_callbacks
        )
        dpo_trainer.generation_config.pad_token_id = self.tokenizer.pad_token_id
        return dpo_trainer


class RLOO(PLLoRATrainer):
    def _pl_trainer_setup(self, train_data, eval_data, iteration, total_training_steps):
        # the data contains the prompt used by other methods, drop it
        train_data = train_data.map(
            remove_columns=['prompt'],
        )
        eval_data = eval_data.map(
            remove_columns=['prompt'],
        )
        pl_config = base2specific_config(
            base=self.base_training_config,
            specific=RLOOConfig,
            name="rloo",
            response_length=self.generation_length,
            local_rollout_forward_batch_size=self.base_training_config.per_device_eval_batch_size,
            num_ppo_epochs=1,
            model_adapter_name="pl",
            ref_adapter_name="sft"
        )
        pl_callbacks = None
        if self.lora_log_frequency is not None:
            pl_callbacks = [
                SaveLoraWeightsCallback(
                    self.policy_model,
                    pl_config.output_dir,
                    iteration=iteration,
                    total_training_steps=total_training_steps,
                    log_frequency=self.lora_log_frequency,
                    adapter_name="pl"
                )
            ]
        rloo_trainer = RLOOTrainer(
            policy=self.policy_model,
            ref_policy=None,
            reward_model=self.reward_model,
            processing_class=self.tokenizer,
            config=pl_config,
            train_dataset=train_data,
            eval_dataset=eval_data,
            callbacks=pl_callbacks
        )
        return rloo_trainer


class ZORLOO(PLLoRATrainer):
    def __init__(
        self,
        *args,
        zopro_learning_rate: float = 1e-4,
        zopro_eps: float = 1e-5,
        zopro_decay_scheduler: str = "linear",
        zopro_ppo_epochs: int = 1,
        zopro_batch_size: int = 32,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.zopro_learning_rate = zopro_learning_rate
        self.zopro_eps = zopro_eps
        self.zopro_decay_scheduler = zopro_decay_scheduler
        self.zopro_ppo_epochs = zopro_ppo_epochs
        self.zopro_batch_size = zopro_batch_size

    def _pl_trainer_setup(self, train_data, eval_data, iteration, total_training_steps):
        # the data contains the prompt used by other methods
        # the data contains the prompt used by other methods, drop it
        train_data = train_data.map(
            remove_columns=['prompt'],
        )
        eval_data = eval_data.map(
            remove_columns=['prompt'],
        )
        if iteration == 0:
            # first iteration use normal rloo, otherwise we don't have data to start with
            pl_config = base2specific_config(
                base=self.base_training_config,
                specific=RLOOConfig,
                name="rloo",
                response_length=self.generation_length,
                local_rollout_forward_batch_size=self.base_training_config.per_device_eval_batch_size,
                num_ppo_epochs=1,
                model_adapter_name="pl",
                ref_adapter_name="sft"
            )
            print("Starting RLOO for first iteration")
            trainer = RLOOTrainer(
                policy=self.policy_model,
                ref_policy=None,
                reward_model=self.reward_model,
                processing_class=self.tokenizer,
                config=pl_config,
                train_dataset=train_data,
                eval_dataset=eval_data,
            )
        else:
            print("Starting actual ZOPrO!!!")
            pl_config = base2specific_config(
                base=pl_config,
                specific=RLOOConfig,
                name="zorloo",
                num_ppo_epochs=self.zopro_ppo_epochs,
                learning_rate=self.zopro_learning_rate,
                per_device_train_batch_size=self.zopro_batch_size,
                per_device_eval_batch_size=self.zopro_batch_size,
                gradient_accumulation_steps=64 // self.zopro_batch_size,
                response_length=self.generation_length,
                local_rollout_forward_batch_size=self.base_training_config.per_device_eval_batch_size,
                model_adapter_name="pl",
                ref_adapter_name="sft"
            )
            # now gather the differences in reward and policy from previous iteration
            # this is needed for ZOPrO
            previous_update = {}
            new_dict = get_peft_model_state_dict(self.policy_model, adapter_name="pl")
            if iteration == 1:
                # load sft model, not pl
                old_dict = get_peft_model_state_dict(self.policy_model, adapter_name="sft")
                for k in old_dict.keys():
                    previous_update[k] = new_dict[k] - old_dict[k]
            else:
                with safe_open(os.path.join(self.log_dir, f'policy_model_iteration_{iteration-2}', 'pl', 'adapter_model.safetensors'), 'pt', device='cpu') as f:
                    for k in f.keys():
                        old_tensor = f.get_tensor(k)
                        new_tensor = new_dict[k].cpu()
                        previous_update[k] = new_tensor - old_tensor
            reward_delta = {}
            new_dict = get_peft_model_state_dict(self.reward_model, adapter_name="reward")
            with safe_open(os.path.join(self.log_dir, f'reward_model_iter_{iteration-1}', 'reward', 'adapter_model.safetensors'), 'pt', device='cpu') as f:
                for k in f.keys():
                    if k not in previous_update.keys():
                        continue  # this is the case for score, etc.
                    old_tensor = f.get_tensor(k)
                    new_tensor = new_dict[k].cpu()
                    reward_delta[k] = new_tensor - old_tensor
            # check they are not all zeros
            if all([torch.allclose(v, torch.zeros_like(v)) for v in previous_update.values()]):
                raise ValueError("Previous update is all zeros, this is likely due to a bug in the code.")
            if all([torch.allclose(v, torch.zeros_like(v)) for v in reward_delta.values()]):
                raise ValueError("Reward delta is all zeros, this is likely due to a bug in the code.")
            trainer = ZORLOOTrainer(
                self.policy_model,
                self.reward_model,
                train_data,
                eval_data,
                self.tokenizer,
                pl_config,
                previous_update=previous_update,
                reward_delta=reward_delta,
                eps=self.zopro_eps,
                decay_scheduler=self.zopro_decay_scheduler
            )
        return trainer


if __name__ == '__main__':
    po_methods = {'ppo': PPO, 'dpo': DPO, 'rloo': RLOO, 'zorloo': ZORLOO}

    parser = argparse.ArgumentParser(description="Train RLHF LoRA model")
    parser.add_argument('--task', type=str, default='chat', help='Task to perform')
    parser.add_argument('--po-method', type=str, choices=po_methods.keys(), default='ppo', help='Policy (Preference) Optimization method')
    parser.add_argument('--model-name', type=str, default='meta-llama/Llama-3.2-1B', help='Name of the model')
    parser.add_argument('--dataset-name', type=str, default='Anthropic/hh-rlhf', help='Name of the dataset')
    parser.add_argument('--lora-r', type=int, default=64, help='LoRA rank')
    parser.add_argument('--lora-alpha', type=int, default=None, help='LoRA alpha. Defaults to 2*r')
    parser.add_argument('--max-length', type=int, default=1024, help='Maximum sequence length')
    parser.add_argument('--log-dir', type=str, required=True, help='Directory to save logs and models')
    parser.add_argument('--batch-size', type=int, default=2, help='Batch size')
    parser.add_argument('--intraepoch-refinement', action='store_true', help='Refine the reward model within the epoch')
    parser.add_argument('--pl-epochs', type=int, default=3, help='Number of Policy Learning epochs')
    parser.add_argument('--lora-log-frequency', type=float, default=None, help='Frequency of logging for LoRA weights. Percentage of training steps.')
    parser.add_argument('--keep-rm-close', action='store_true', help='Keep the reward model close to the previous iteration')
    parser.add_argument('--rm-alpha', type=float, default=0.5, help='Weight for the previous reward model')
    parser.add_argument('--iteration-percentage', type=float, default=0.05, help='Percentage of the dataset to yield for each iteration')
    parser.add_argument('--learning-rate', type=float, default=5e-5, help='Learning rate for the models')
    parser.add_argument('--generation-length', type=int, default=53, help='Length of the generated completions')
    parser.add_argument('--resume', action='store_true', help='Resume training from the last iteration. Only works if the log directory contains the last policy checkpoint and relative data.')
    # ZOPrO specific arguments
    parser.add_argument('--zopro-learning-rate', type=float, default=1e-4, help='Learning rate for ZOPrO')
    parser.add_argument('--zopro-eps', type=float, default=1e-5, help='Epsilon for ZOPrO')
    parser.add_argument('--zopro-decay-scheduler', type=str, default='linear', help='Decay scheduler for ZOPrO')
    parser.add_argument('--zopro-ppo-epochs', type=int, default=1, help='Number of PPO epochs for ZOPrO')
    parser.add_argument('--zopro-batch-size', type=int, default=32, help='Batch size for ZOPrO. Since the method use much less memory, this can be increased from the other batch size value.')
    args = parser.parse_args()

    assert not (
        "llama" in args.model_name.lower()
        and "Skepsun/cvalues_rlhf" in args.dataset_name
    ), (
        "Dataset is in chinese but llama doesn't support chinese. Use a different model or a different dataset."
    )
    trainer = po_methods[args.po_method](
        model_name=args.model_name,
        dataset_name=args.dataset_name,
        task=args.task,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        max_length=args.max_length,
        log_dir=args.log_dir,
        batch_size=args.batch_size,
        intraepoch_refinement=args.intraepoch_refinement,
        keep_rm_close=args.keep_rm_close,
        keep_rm_alpha=args.rm_alpha,
        lora_log_frequency=args.lora_log_frequency,
        pl_epochs=args.pl_epochs,
        iteration_percentage=args.iteration_percentage,
        learning_rate=args.learning_rate,
        generation_length=args.generation_length,
        zopro_learning_rate=args.zopro_learning_rate,
        zopro_eps=args.zopro_eps,
        zopro_decay_scheduler=args.zopro_decay_scheduler,
        zopro_ppo_epochs=args.zopro_ppo_epochs,
        zopro_batch_size=args.zopro_batch_size
    )
    print("Starting general training with following configuration: ", args.__dict__)
    trainer.train(resume=args.resume)
